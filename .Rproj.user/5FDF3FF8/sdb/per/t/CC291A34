{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Appendix 6, CLD3 Processes And Platforms Framework\"\noutput:\n  bookdown::html_document2:\n    toc: yes\n    pandoc_args: [\n      \"--number-offset=6\"\n    ]\nbibliography: [CLD3ProcessesAndPlatforms.bib]\n---\n\n```{r echo=FALSE}\nlibrary(bookdown)\n```\n\n# {-}\n\n## Purpose & Approach\nA significant amount of data relevant to evidence-based policymaking is collected at the local community level (e.g. education, housing, zoning, traffic, emergency services, social services).  Existing local level data are a rich source of information that can be repurposed to support community-level decision-making. However, many communities lack the resources and technical expertise (i.e. the ability to clean, re-purpose, store and analyze data) to take advantage of decision-making enhancements to be found in their own local administrative data.\n\nTo overcome that barrier, we are creating an open source loosely federated data infrastructure of processes and supporting platforms that can be adopted and adapted by other states. The sustainable strategy is for the state Land Grant Universities to become the stewards of this infrastructure ensuring it is accessible by local communities, researchers, and state agencies. This data infrastructure will manage and mediate access to data and data tools, adhering to necessary data privacy, confidentiality, security and data governance constraints.  The associated technology will enable greater linkage of data capable of adhering to required privacy and security protections. Researchers across many academic departments, who want to help local or state government solve problems, would be able to find government partners and utilize the coordinated data infrastructure being curated by the Land Grant University to the benefit of their research and the government partner. This federated CLD3 Data Access Processes & Platforms Infrastructure will become part of the basic infrastructure of each State Land Grant University, similar to a library system.  Figure 1 provides a view of the data processes and platforms that need to be developed, integrated, and evolved.\n\n### Culture of Community Learning\nWhat is called for is an approach to community-relevant data that goes beyond simply liberating the data once and conducting one-off analyses. The purpose instead needs to focus on enabling communities themselves to be able to carry out the necessary steps to re-purpose their own data, combine their data with other (e.g. Federal, State) community-relevant datasets, conduct analyses of community-relevant hypotheses, and produce findings in a manner that can be easily used for evidence-based policy and decision-making. In short, we need focus on enabling and fostering, at the level of the locality, a \"Culture of Community Learning\".\n\n### Community-Relevant vs Community-Resident Data\nCentral to the CLD3 approach is the recognition that to fully engage in community learning, a focus on locality level data alone is limiting.  County, State and Federal data sources can inform individual localities.  In fact, fundamental to CLD3 is the re-conceptualization of thinking about where data is generated to a focus on the degree to which a data source is locality useful [community useful?], no matter the source (e.g., federally generated data on a single census-tract is potentially high on its locality usefulness).  The CLD3 process and platforms have been developed with an eye towards full-inclusion of all data.  The concept of locality usefulness simply normalizes data w.r.t the community of interest.\n\n### Focus on Technological Heterogeneity\nThe differences between different localities in the same state in terms of technological capabilities can be stark. These differences tend to vary predictably with differences in population and tax-base. What this means, from a data persepctive, is that we want to integrate datasets from mutliple localities we are geenrally faced with data being stored in multiple formats, organized in multiple ways, needing to be accessed via multiple means. In short, when dealing with multiple localities, a heterogeneous data environment is de rigueur.\n\n```{r CLD3Framework-fig, out.width='90%', fig.cap='CLD3 Process and Platform Framework', echo=FALSE}\nknitr::include_graphics(rep(\"images/CLD3Framework.png\"))\n```\n\nThe most common pitfall of the well-intentioned data system that has been created to aid multiple public actors in integrating their data is the assumption that the public actors are able to, or will want to, modify their existing data storage and management procedures to facilitate their data being included in the new system. These one-size-fits-all initiatives have been numerous and can especially be seen as theoutputs of many a university research project. If we hope to enable data-driven learning at the community-level, we must instead begin by accepting the heterogeneous data environment as the 'normal' data environment. And, additionally accept that this circumstance will mostly likely NOT change (at least not in any timely manner). Accepting a heterogeneous data environment means, therefore, dictates the need to provide a heterogenous menu of methods to access and use locality data that will allow for the least possible amount of effort on the part of the locality to get you the data. For some localities this could mean...\n\n### Scalable, Transferable, and Cost-Effective Data Management, Analytics, and Presentation Platforms\nWhile every community is offered the use of all CLD3 platforms, many will only need particular modules in accordance with community needs and community capabilities. Built using the latest open-source data technologies, individual CLD3 platforms can be hosted locally or in the cloud. A set of Virtual Private Servers (VPSs) is dedicated to each local data partner. As the data management and analysis expertise of a community grows, it may become desirable for that community to have a more direct controlling role over the technology platforms. In fact, such developments are a sure sign of success in capacity-building efforts with CLD3 data partners. The CLD3 plaforms are purposely designed and constructed for ease of transferability for just such a circumstance. The computer code that underlies the CLD3 platforms is open-source and thus will be made available to any community under GNU Public License along with documentation. \n\n## CLD3 Processes & Platforms Framework Details\n### Community Discovery\nThe most critical steps in the successfull deployment of the CLD3 platform for use by localities are the ones taken to establish effective engagement with and discovery of the partner community. Accordingly, the CLD3 community engagement process facilitates discovery by conducting issue identification activities and systems analysis to provide an adaptable initial sketch of community status (e.g., distribution of resources and capabilities). During systems analysis, the CLD3 approach employs various proven engagement methods to aid in determinig the critical aspects involved in a community's successful participation in and use of CLD3 technology platforms. Such critical aspects include the state of data management technology currently employed, capacity to bring analytics to bear on the data, and the fitness of the data itself for use in such analyses. Once data discovery has commenced, the CLD3 process affords rapid prototyping (e.g., of potential data products) during the data discovery process (i.e., a quick assessment of the current state of the community and/or assessment of fitness of current data streams).\n\n#### Data Management System Status Discovery\n\n#### Assessment of Data Analytics Capabilities\n\n#### Data Discovery & Inventory\nData discovery is the identification of potential data sources that could be related to the specific topic(s) of interest. Data inventory is the process by which sources of data are screened to determine their potential usefulness in supporting the research questions. Those that pass the screening and deemed worthwhile and in need of additionapl profiling.\n\n##### Identification\nThe first problem we run into when determining appropraite data targets is who to interview? One approach is to start by simply brainstorming possible data sources with the goal of thinking as broadly and imaginatively as possible in order to assemble a list of potential sources. A more deliberative approach would be to start with a technique like quota sampling. Following this initial identifcation step, we can then procede with another technique called snowballing. The purpose of the initial quota sample is to make sure that you start with a set of interviewees that represent the most obvious set of data stakeholders. This approach is followed by snowballing, however, because it is quite unlikely that you will ‘guess’ all of the appropriate data stakeholders at the beginning, and you will need to solicit the opinion of the first interviewees as to how to proceed.\n\n###### Quota Sampling\nAlthough quota sampling is a non-probability method, it is similar to probability sampling. Like a probability sampling method, quota sampling ‘addresses the issue of representativeness, although the two methods approach the issue quite differently’ [@babbie1998practice], p 196. How does quota sampling work? First, the researcher begins with ‘a matrix, or table, describing the characteristics of the target population. If the researcher needed a national quota sample, for instance, she “would need to know what proportion of the national population is urban, eastern, male, under 25, white, working class, and the like, and all the other permutations of such a matrix” (196). After constructing this matrix and assigning a relative proportion assigned to each cell in the matrix, the researcher should collect data from people having all the characteristics of a give cell. The researcher could then go farther and assign ‘a weight’ to each person in each cell ‘appropriate to his or her portion of the total population.’\n\n###### Snowballing\nSnowballing is a popular technique used in network studies [@wasserman1994social]), particularly in situations where stakeholders or other interested representatives are not easily identifiable [@goldenberg1992social]. Hence, this strategy is extremely useful in helping identify all relevant data stakeholders. Snowballing is a simple process of expanding the zone of contacts through initial contacts. The process begins by identifying an initial group of stakeholders, either those already involved in the preliminary stages of the process or those identified via another mechanism (e.g. quota sampling). These actors or participants are then asked to identify those individuals whom they feel should be involved in the process as well. This is the “first-order” zone. The researcher then proceeds to contact those actors (whether individuals or groups) and proceeds to have these “second-order” actors, further identify others who they think would have an interest in the project or process ([@wasserman1994social, p. 34; also @goldenberg1992social; @babbie1998practice; @doreian1992fixed]).\n\nThe process can involve asking respondents to review a “fixed list” from preliminary research or to simply brainstorm to identify those stakeholders that they think are important to add to the list of those whom they think should be included. If being asked to brainstorm, it is advisable to precede this task with another that may stimulate thinking. The key to this process is to be as exhaustive as possible. The purpose of snowballing here is to find, as quickly as possible, the self-limiting reference system of the new network of dataset stakeholders. By self-limiting we mean that after a few iterations of snowballing, the names of suggested new interviewees begin to be repeated. When most of the names suggested have already been interviewed, you have reached the end of the snowball process.\n\n##### Data Source Screening and Inventory\nThe first step before conducting a full data inventory is to screen the data sources, identifying which sources are worthy of a deeper look and which are worthy of consideration for profiling. The screening includes five questions and a qualitative evaluation of purpose, data collection method, selectivity, accessibility, and description. Figure \\@ref(fig:DataSourceScreening-fig) shows a sample screening instrument from a recent project.\n\n```{r DataSourceScreening-fig, fig.cap='Sample Data Source Screening Instrument', echo=FALSE}\nknitr::include_graphics(rep(\"images/DataSourceScreening.png\"))\n```\n\nFollowing an initial screening inventory, a subset of the sources are selected for a full inventory. Figure \\@ref(fig:DataSourceInventory-fig) shows a sample inventory instrument.\n\n```{r DataSourceInventory-fig, fig.cap='Sample Data Source Inventory Instrument', echo=FALSE}\nknitr::include_graphics(rep(\"images/DataSourceInventory.png\"))\n```\n\n#### Data Aquisition Plan\n\nFollowing these discovery processes, necessary data agreements (e.g. MOAs, MOUs) and a data acquisition plan can be drafted and agreed upon.\n\n### Data Management Processes & Platform\nThe CLD3 Data Management Process is comprised of two distinct sub-processes: Data Ingestion and Data Storage & Administration.\n\nData Ingestion is the process of bringing data into the CLD3 system, specifically into the Data Management Platform.   The variety of ingestion processes will be as diverse as all the potential data sources and will account for a broad and heterogeneous spectrum of required access requirements, protocols, data formats, schemas, and update frequencies.  Broadly speaking, data ingestion processes can be divided into two categories: batch and streaming. Ingesting batch data involves importing data in discrete chunks, such as dumps of a day’s transactions. Streaming ingestion (sometimes referred to as real time) means every data item is ingested individually, immediately (or close to) as it is emitted by the source, such as sensor outputs (e.g. pollution sensors) or social network messages (e.g. Twitter). The ingestion of batch data is the primary focus of the CLD3 process.  Although, dynamic querying of data sources as needed (data federation) could possibly be conceptualized as a special form of streaming. The details of the ingestion process are provided in Figure \\@ref(fig:DataIngestionPlatform-fig) (as well as in Figure \\@ref(fig:CLD3Framework-fig) under “Data Management”) and are:\n\n- Establish Access Type & Method\n    - Push, Pull, or Leave\n- Select Best Protocol for Type and Method\n    - SFTP, secure USB via sneakernet, secure dropbox\n    - Provider API (i.e. REST)\n    - VT SAFR-Data Adapter for secure privacy protection\n- Marshall Data Access Platform (Apache NiFi)\n    - SFTP, secure USB via sneakernet, secure dropbox\n    - Establish System Mediation Logic\n    - Establish Data Pipeline Processes & Transformations\n    - Establish Data Schedule\n    - Establish Data Provenance Maintenance\n- Store\n    - Each research project stored on a new project-dedicated encrypted partition (LUKS-on-LVM) on secure servers\n    - Original Data is Stored as Non-removable & Non-Editable\n\n```{r DataIngestionPlatform-fig, fig.cap='Heterogeneous Data Ingestion Platform', echo=FALSE}\nknitr::include_graphics(rep(\"images/DataIngestionPlatform.png\"))\n```\n\nThe CLD3 data ingestion process is supported by technologies configured to work with a heterogeneous set of potential data sources. For data sharing arrangements that require the pushing of data into the CLD3 data platform, Secure FTP (SFTP) and a secure web-based dropbox are supported.  For data sharing arrangements that require the pulling of data into the CLD3 data platform, REST-based and SOAP-based APIs, WebDAV, HTTPS, and customized scripting for web-scraping are all supported. For data sharing arrangements that require leaving data in the host system and dynamically querying the data as needed (federated data), CLD3 again supports both REST-based and SOAP-based APIs, but also is able to deploy at the data partner’s location our own custom technology for establishing dynamically queryable data sources.  The technology deployed at the partner location is the SAFR-Data Adapter that provides a secure, encrypted tunnel and presents only the data elements agreed to by the data partner. In addition, if needed, any personally identifying information can be encrypted with a custom one-way hash designed specifically to still be linkable to other datasets [@schroeder2012pad]. The supported protocols are employed by several data management platform technologies including Apache NiFi (a dataflow automation system designed by the NSA and recently made open-source and transferred to the Apache Foundation), Apache Spark (which is quickly developing many of the same data ingestion capabilities), and the SAFR-Data Shaker for the secure dynamic querying of federated data sources (in conjunction with the SAFR-Data Adapter)[@schroeder2013shaker; @schroeder2012adapter; @schroeder2013ident; see Figure \\@ref(fig:SAFRDataAdapterShaker-fig)].\n\n```{r SAFRDataAdapterShaker-fig, fig.cap='SAFR-Data Adapter & Shaker', echo=FALSE}\nknitr::include_graphics(rep(\"images/SAFR-DataAdapter.png\"))\n```\n\n### The Lexicon - Metadata Storage, Management, Linkage and Provenance\nAt its base, the CLD3 Lexicon serves the function of a metadata repository—a database created to store metadata from various systems. Metadata is information about the structures that contain the actual data. Metadata is often said to be \"data about data\", but the Lexicon goes far beyond this definition, proving a centralized node of data source information that can be used for provenance tracking and data linkage within a heterogeneous network of data sources [@schroeder2013shaker]. Specifically, the Lexicon is an inventory of and history of changes to:\n\n- every available data field in every available data source\n- the structure of their storage\n- possible values and meanings of the information stored\n- possible transformations of each set of field values from one data source to another set of field values from another data source\n- methods of data source access\n- matching algorithms and how they are to be used in conjunction with possible field value transformations\n\nThe Lexicon provides fundamental functions for the operation of the CLD3 Framework.  It is a requirement that all data partners in the network provide the data information necessary for its operation. The Lexicon is maintained in an RDBMS by CLD3 staff, thus removing the complexity required for high quality data linkage from all data partners (i.e., a standardization scheme enforced among data partners).  \n\nFigure \\@ref(fig:LexiconTables-fig) shows an example of an entry in the Lexicon. In the CLD3 all data source are assigned three entries that capture the metadata associated with the entry (the three top boxes in Figure 5 show this; one box for metadata on the data source, one for metadata on sub-components of the data source columns, and one that captures the metadata for all possible valid values in each column).  Whenever two data sources are joined, there are three associated entries in the lexicon that manage the mapping across data sources (see the lower 3 boxes in Figure \\@ref(fig:LexiconTables-fig)).\n\n```{r LexiconTables-fig, fig.cap='Lexicon Tables Example', echo=FALSE}\nknitr::include_graphics(rep(\"images/LexiconTables.png\"))\n```\n\n### Analytics Process & Platform\nThe driving goal of the data analytics process is understanding the fitness-for-use in analysis of provided data. More precisely, and in terms of CLD3, the iterative goals of the analytics platform is the development of a disciplined process for profiling data sources in terms of data quality [@keller2016evolution], structure and metadata; preparing selected data for analysis via cleaning, transformation and restructuring; creating linked datasets at the highest possible level of accuracy; testing the prepared dataset(s) as adequate for use in whatever analyses/modelling proposed; and, finally, testing proposed hypotheses and producing analyses and other tools.\n\nThe CLD3 analytics process can be conceptualized as being comprised of three semi-distinct sub-processes: Data Fitness Analysis, comprised of Data Profiling, Data Preparation (e.g., cleaning, transformation, restructuring), Data Linkage [@schroeder2013ident], and Exploratory Analysis activities; Data Analysis & Hypothesis Testing, comprised of Experimental Design, Modeling, and Analysis activities; and, the Creation of Community Data Products, such as problem-specific algorithms, geographic visualizations, and new community indicators.\n\nAll of the analytic sub-processes are supported by a platform providing the latest technologies (open-source) including the RStudio Server development environment for working with both R and Apache Spark, the Jupyter Notebook Server (Jupyterhub) development environment for working with Python, R, and Apache Spark, the SAS Studio development environment for working with SAS (note, SAS is not open-source and is not a necessary part of the CLD3 platform), and Adminer, PGAdmin, R, and Python interfaces for working with a backend PostgreSQL RDBMS. All of the platform technologies are administered via the latest container technologies (i.e. lxc, docker). Doing so allows for the easy movement and management of separate platform technologies to alternative hosting environments (e.g. Amazon Web Services, Google Cloud) when required (e.g. setting up a locality to manage its own analytics platform). \n\nFigure \\@ref(fig:SecureTunnelAccess-fig) illustrates the flow of information within and outside of the CLD3 platform during the analytic process and has the following key functions:\n\n- No direct connections by data analysts to data servers\n- All data connections via web-interfaced data applications\n- All connections to data applications via Secure Shell (SSH) using asynchronous rsa key-pairs\n- SSH used to establish forwarded ports from workstations to specific data application servers\n- Communication from Application Server to outside the host server is not possible\n- Authorized data administrators may mount fuse-based file systems via sshfs for the purpose of data transfer and management\n\n```{r SecureTunnelAccess-fig, fig.cap='Information Flow within and outside of the CLD3 platform during analytics processing', echo=FALSE}\nknitr::include_graphics(rep(\"images/SecureTunnelAccess.png\"))\n```\n\n#### Data Fitness Analysis\nThe purpose of developing a data science process in the context of specific problems is to find synergies across application domains with respect to the data science process and use. The ultimate goal is to develop a disciplined process of identifying data sources, preparing them for use, and then assessing the value of these sources for the intended use(s).\n\nUnderstanding how to approach fitness for use starts with considering the modeling and analyses that will use the data. Modeling depends on the research questions and the intended use of the data to support the research hypotheses. Fitness assessment should be about the fitness of the data for the modeling, from straight forward tabulations to complex analyses. Therefore,fitness is a function of the modeling, data quality needs of the models, and data coverage (representativeness) needs of the models. Finally, fitness should characterize the information content in the results.\n\n##### Data Profiling\nData profiling starts with a determination of both the quality of the data and its utility to the project at hand. There are numerous rubrics available, but a useful initial assessment of data quality can be achieved through statistical analysis of data field completeness, data field value correctness, and logical consistency between fields and between records (Redman, Wang, DOD, others). In addition, it is useful to ascertain the spread of unique values within a field, as well as the rate of duplication at the record level. For an assessment of dataset utility, the dataset's structure should be analyzed. This is to determine how well the dataset has been structured for the purposes of the intended analyses. The the state of the dataset's metadata should also be analyzed to determine how well observational units and their attributes are defined. An important feature of the data profiling process is that discovered issues are only described and not actually \"fixed\". The appropriate fix will depend upon the specific needs of the research. If the prescribed \"fix\" is not appropriate or even possible there would be no need for any action and attempting a fix at this stage could result in wasted time and effort. For example, given a community housing file, it may be more appropriate to simply normalize city zoning entries into \"residential\" or \"non-residential\" versus painfully re-categorizing every missing or incorrectly entered value into 38 different zoning classfications.\n\n###### Data Quality & the Dimensions of Quality - Does the data correctly represent the real-world construct to which it refers?\nA considerable amount of data quality research involves investigating and describing various categories of desirable attributes (or dimensions) of data. These lists commonly include accuracy, correctness, currency, completeness and relevance, as described in Chapter 2. Nearly 200 such terms have been identified and there is little agreement in their nature (are these concepts, goals or criteria?), their definitions or measures (Wang et al., 1993). Here we have let a typology emerge form the project data work. This typology consists of five data quality areas: completeness, value validity, consistency, uniqueness, and duplication. Regardless the typology chosen, the final judgment of data quality is measured by adherence of a dataset to a set of data quality rules. \n\n###### Data Structure - Is the data appropriately structured for purposes of analysis?\nBecause datasets are often created for reasons of administration and reporting, and further that there are generally few constraints on their organization, administrative datasets are often constructed in manners not conducive to statistical analyses. During the data profiling step, issues about data structure are identified. During the data transformation step decisions on how to restructure data are made and executed. An example from the housing case study is given in Figure \\@ref(MLScombine-fig). The dataset provided was comprised of single records with 128 fields. All fields in the record were keyed to the variable``List Number.'' Structured this way, it is not possible to analyze property changes over time, even though the data does in fact exist within the dataset. That is, a restructuring is necessary that pulls out and re-relates the property information to a different key (here, Parcel ID). This situation occurs often as a result of the structural issue 'combined observational unit types' discussed below. \n\n```{r CombinedTypes-fig, fig.cap='Combined data types of List Number and Parcel ID from MLS data table.', echo=FALSE}\nknitr::include_graphics(rep(\"images/CombinedTypes.png\"))\n```\n\n###### Metadata: Are datasets, observation units, and their attributes consistently named, sufficiently described, and appropriately formatted for combination with other datasets?\n\nMetadata is generally defined \"data that provides information about other data\". [http:\\/\\/www.merriam-webster.com\\/dictionary\\/metadata]. The main purpose of metadata is the facilitation of the discovery relevant information pertaining to a particular object/resource. It does this by \"allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information.\"[guenther2004understanding] Therefore, generally speaking, a lack of metadata for a dataset can present significant impediments to the use of said dataset. Being more specific, when dealing with data that is to be used for research purposes, it is of vital importance to discover if the datasets (tables), their observational observation units (records/rows), and their attributes (fields/columns) are consistently named, sufficiently described, and appropriately formatted for analysis and for combination with other project datasets. Additionally, does information exist regrading any transformations that have occurred to original data sources in the creation of said dataset, as well as who did the transforming?\n\n##### Data Preparation\n\n##### Data Linkage\n\n##### Data Exploration & Characterization}\nData exploration & characterization refers to the analysis of datasets by summarizing main characteristics, often with visual methods. Data exploration is used throughout the data framework in an iterative manner. Descriptive statistics play a principal role in data profiling, from identifying valid attribute values to checking for semantic consistency. The use of visual techniques like boxplots support iterations between data cleaning and transformation during data preparation. Additionally, distributional characterizations of the data help identify needs and opportunities for data linkage.\n\n#####\tData Analysis & Hypothesis Testing\n######\tExperimental Design\nThe experimental design follows from the hypothesis being tested.  It describes how to collect the data so that the maximum amount of information can be obtained from the available resources in the most efficient manner.  The experimental design dictates how the data are analyzed.\n######\tExploratory and Inferential Data Analysis\nExploratory data analysis employs graphical and descriptive statistics to tell the story of a set of data.  Inferential statistics are used to extend that story from the set of data (sample) to the larger population.  \n######\tAssumption Checking\nThe inferences from an experimental design are valid only if the assumptions of the experimental design and the model used to describe the experiment are met. Exploratory and inferential methods are used to check the assumptions such as the normality of the model residuals, the presence of outliers, and homogeneity of variance.\n\n#####\tCreation of Community Data Analysis Tools \n######\tStatistics\nA catalog of experimental designs for policy interventions, that will include: how to implement the design, the associated assumptions, appropriate statistical methods for analyzing the data, the inferences that can be made, and examples of how the design has been used for policy interventions. Also included, will be a review of the research on experimental designs for policy interventions. The research will be conducted and monitored on the statistical methods that capture the impact of the unintended consequences of policy interventions. \n######\tVisualization\nA catalog of static and interactive ways to visualize univariate, multivariate, and spatial information.  Also included will be a list of visualization resources and algorithms.\n######\tIndicators\nA catalog of indicators that reflect the interplay between social, environmental, and economic factors of a community’s well-being.  The catalog will include: a list of indicators, the data sources that can be used to estimate each indicator, a list of the empirical research on each indicator, and research into how to quantify the correlation structure between indicators.  Also included will be information on the community indicator projects across the country and a list of community indicator resources.\n\n```{r ShinyDashboard-fig, fig.cap='CLD3 Sample Dashboard using RStudio Shiny', echo=FALSE}\nknitr::include_graphics(rep(\"images/ShinyDashboard.png\"))\n```\n\n### Data Presentation Process & Platform\nThe CLD3 Process for Data Presentation includes a myriad of activities including report preparation, website building, dashboard building, establishing wikis. The CLD3 Platform provides a number of technological tools to support these activities, including an NGINX web server, an RStudio Shiny server (built via RStudio), and a Confluence Wiki server.  Figure ref{fig:ShinyDashboard} illustrates the value of the Shiny Dashboard, a dynamic graphical interactive interface that affords rapid data exploration for decision making and community involvement.  Figure \\@ref(fig:ShinyDashboard-fig) explores the age-dependent access to supermarkets (lower graphic); the map shows the geolocation of all supermarkets in the Commonwealth of Virginia; the color of counties capture the % of the county with low access to supermarkets). Notice that the left-hand panel has dynamic options that, when engaged, change the nature of the graphics.\n\n### Policy-Based Permissions Engine\nThe Policy Engine employs a rule-based access system to enforce the data access restrictions and requirements dictated by applicable federal, state and local rules and regulations. Within the CLD3 framework, the policy engine functions as what may be described as a “data resource broker”.\n\n```{r iRODSArchitecture-fig, fig.cap='integrated Rule-Oriented Data System (iRODS) Example Architecture', echo=FALSE}\nknitr::include_graphics(rep(\"images/iRODSArchitecture.png\"))\n```\n\nA data resource broker provides a uniform interface to heterogeneous computer data storage resources over a network or networks. As part of this, a data resource broker typically implements a logical namespace (distinct from physical file names) and maintains metadata on data-objects (files), users, groups, resources, collections, and other items in a metadata catalog stored in a relational database management system. This metadata can be queried to locate files based on attributes as well as by name across heterogeneous data resources. A data resource broker will generally have features to support the management and collaborative (and controlled) sharing, publication, replication, transfer, and preservation of distributed data collections. The CLD3 system will employ the integrated Rule-based Data management System (iRODS) (see Figure \\@ref(fig:iRODSArchitecture-fig)), working in combination with the Lexicon, to provide necessary data resource broker functions.\n\n## CLD3 Data Management Plan - Transfer, Storage, Access, and Destruction\n### Transfer\nAll data sets transferred to SDAL are done so via encrypted means. For remote transfers, a certain set of network protocols is acceptable. Acceptable network protocols for remote data transfer include FTPS, SFTP, SCP, and WebDAV over HTTPS. The SDAL preferred remote transfer method is via an SFTP server temporary established for the purpose of the transfer. The SFTP server is deactivated after successful data transfer. The remote user is provided a temporary password for SFTP server access that is also deleted after successful data transfer. Data sets that are to be transferred manually (not via remote network connection) are transferred using an encrypted USB storage device employing, at a minimum, an EncFS-based encrypted file partition.\nEmail is not considered secure and is not used to transmit covered data unless additional file-level encryption tools, requested and approved by the data provider, are employed (e.g. gpg, ccrypt, 7zip using AES).\n### Storage\nThe data for each SDAL research project is stored on a new project-dedicated encrypted Logical Volume Management (LVM) partition on one of our servers. The LVM partition is encrypted using Linux Unified Key Setup or LUKS. This setup is typically referred to as “LUKS-on-LVM”. LUKS is a disk-encryption specification that is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. Each SDAL data server is housed within one of three highly-secure, high-performance computing (HPC) data centers located in the Virginia Bioinformatics Institute (VBI) in Blacksburg, VA and maintained by VBI Information and Technology Computing Services personnel. Direct access to data sets for loading and management purposes is restricted to project-approved PIs and data managers. The method of access for loading and management purposes is via SSH using RSA encrypted key pairs.\n### Access\nCopying of server-hosted data down to researcher workstations is not permitted. All data set analyses and results are saved back to the server, not to the individual researcher’s workstation. Originally provided data records are accessible via secured remote access only in a read-only format to authorized users. The intention is that original data sets are never modified. Resulting modified read-write data sets that are produced from the original data sets are also stored back to the server and only accessible via secured remote access. Unless special authorization is given, researchers do not have direct access to data files. Instead, data access is mediated by the use of different data analysis tools hosted on their own secure servers that connect to the data server via rsa key-pair authenticated ssh-tunneling on non-standard ports (e.g. ssh -L 54000:localhost:5432 for connection to a PostgreSQL server). For example, if a researcher is going to use the R statistical system to analyze the data records, the researcher will log into the RStudio Server. From RStudio Server, which has been granted limited access to file-based data sets and data records stored in a database, the researcher will run the required analyses. Results of the analyses can only be saved to the researchers home directory on the hosting server. Researchers are not permitted to save analysis results to their office workstations.\n### Destruction\nAs required by specific data use agreements, completed project data can be either securely archived on encrypted VBI backup servers or destroyed. For data that is to be destroyed, the encrypted LVM partition on which the data is housed is first wiped using the Unix ‘shred’ utility, overwriting all partition data files three times with multiple patterns by default. After the data is overwritten, the encrypted LVM partition is then destroyed.\n\n## Related Efforts\n### Allegeny\nRelated efforts to date have yet to realize the potentials of the CLD3 process.  Perhaps the closest in spirit is the work in Allegheny County, embedded in the Department of Human Services, that provides cross-sectoral data sharing for purposes of both more efficient and efficacious county management and improved outcomes for county residents who are implicated in the Allegheny DHS system. Allegheny stands out as an example with a high-degree of buy-in from a broad swath of stakeholders and strong, consistent leadership at the county level.  However, although much can be learned from this effort, there are some key differences when contrasted with CLD3, the most prominent of which is that CLD3 emphasizes all data (not just local governmentally generated data) and is tied directly to the communities, both in terms of pinpointing issues and in terms of developing workforce capacity for improving the use of data for community needs. \n\nThe Allegheny County example is contrasted by a recent and ongoing effort in Houston, Texas (Kinder Institute-Transformative Urban Data Platform) that focuses on developing a data warehouse for academic research into issues relevant to urban areas.   The data warehouse approach compared to the CLD3 approach, is much less nimble in terms of addressing community needs, in part because the data is generated before the problems are identified (i.e., exiting data leads to identification of problems that are addressable by existing data).  This type of effort is more common in the community learning space.\n\n### Kinder-Institute\nThe Allegheny County example is contrasted by a recent and ongoing effort in Houston, Texas (Kinder Institute-Transformative Urban Data Platform) that focuses on developing a data warehouse for academic research into issues relevant to urban areas.   The data warehouse approach compared to the CLD3 approach, is much less nimble in terms of addressing community needs, in part because the data is generated before the problems are identified (i.e., exiting data leads to identification of problems that are addressable by existing data).  This type of effort is more common in the community learning space.\n\n### Health\n#### Public Health\nPublic health has been and is still a leader in using data that is locality useful for improvement of community health (e.g., RWJF county health rankings; joint CDC/RWJF 500 Cities initiatives; All in; PRISM; YMCA-Healthy Communities).  In general, outside of patient health information, the health data systems that are integrated are at the state and federal level for the purposes and use of local communities.  These efforts are really leveraging and calling for increased efforts at the federal level to provide more temporal and spatial granularity to the public health community.  This is a good notion, but not in the spirit of CLD3 in which the focus is on data from all sources that is locality useful. \n\n#### Clinical Trials\nClinical trials in medicine, although pushing for data sharing across trials, is a field that focuses on issues strictly related to improving scientific understanding and implementation of clinical findings (NEJM 2016).  The focus is on communities that are defined by disease states alone.\n\n### Sensor Networks\nAnother related thread is what is called participant sensing for which IoT and AoT networks generate community level data (REF Making Sense Report) for specific community needs.   These types of efforts, because of the potential flexibility and ability to address changing community needs, are clearly in the spirit of CLD3.  However, there is little harmonization with all potential data sources that are locality useful for a specific need.  \n\nIn sum, all of these threads serve specific, important needs.  The CLD3 platform is designed as a more general purpose, flexible framework for changing communities for the better—a unique part of the ecosystem that will both compliment and integrate with current and future approaches\n\n## References\n",
    "created" : 1478890475668.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4015871570",
    "id" : "CC291A34",
    "lastKnownWriteTime" : 1478377025,
    "last_content_update" : 1478377025,
    "path" : "~/git/CLD3ProcessesPlatforms/CLD3ProcessesAndPlatforms.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}